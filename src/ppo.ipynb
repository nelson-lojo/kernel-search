{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/project/chipyard/.conda-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from datasets import Dataset\n",
    "\n",
    "from train import evaluate_out, BASE_EXO, CodeGen7\n",
    "\n",
    "NUM_ITERATIONS: int = 10\n",
    "# cg25_7 = CodeGen7()\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\" # \"Salesforce/codegen25-7b-mono\" # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]\n",
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Meta-Llama-3-8B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=1,\n",
    "    ppo_epochs=NUM_ITERATIONS,\n",
    "    mini_batch_size=1,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 68.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_dict({ \"query\": [BASE_EXO] })\n",
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    # optimizer=torch.optim.SGD(model.parameters(), lr=config.learning_rate),\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards(responses: list[str]) -> list[Tensor]:\n",
    "    rews = [ ]\n",
    "    measured = { }\n",
    "    for resp in responses:\n",
    "        if resp in measured.keys():\n",
    "            rews.append(measured[resp])\n",
    "            continue\n",
    "        valid, cost = evaluate_out(resp)\n",
    "        rew = torch.tensor(-cost if valid else -1e10)\n",
    "        rews.append(rew)\n",
    "        measured[resp] = rew\n",
    "    return rews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': ['from __future__ import annotations\\nfrom exo import proc, DRAM\\nfrom exo.stdlib.scheduling import *\\n\\n# Algorithm definition\\n@proc\\ndef generated_operation(\\n    In: i8[16, 16] @ DRAM,\\n    Weights: i8[16, 16] @ DRAM,\\n    Out: i8[16, 16] @ DRAM,\\n):\\n    for i in seq(0, 16):\\n        for j in seq(0, 16):\\n            for k in seq(0, 16):\\n                Out[i, j] += In[i, k] * Weights[k, j]\\n\\n# Now we create a schedule to improve performance!\\n## first switch to \"output stationary\"\\ngemmini = rename(generated_operation, \"generated_operation_scheduled\")\\ngemmini = reorder_loops(gemmini, \"j k\")\\ngemmini = reorder_loops(gemmini, \"i k\")\\n\\n## then '], 'input_ids': [tensor([1527], device='cuda:0'), tensor([1328], device='cuda:0'), tensor([21733], device='cuda:0'), tensor([565], device='cuda:0'), tensor([1179], device='cuda:0'), tensor([33307], device='cuda:0'), tensor([198], device='cuda:0'), tensor([1527], device='cuda:0'), tensor([506], device='cuda:0'), tensor([78], device='cuda:0'), tensor([1179], device='cuda:0'), tensor([13988], device='cuda:0'), tensor([11], device='cuda:0'), tensor([14644], device='cuda:0'), tensor([1428], device='cuda:0'), tensor([198], device='cuda:0'), tensor([1527], device='cuda:0'), tensor([506], device='cuda:0'), tensor([78], device='cuda:0'), tensor([13392], device='cuda:0'), tensor([2808], device='cuda:0'), tensor([516], device='cuda:0'), tensor([45456], device='cuda:0'), tensor([1179], device='cuda:0'), tensor([20386], device='cuda:0'), tensor([2], device='cuda:0'), tensor([41425], device='cuda:0'), tensor([7419], device='cuda:0'), tensor([198], device='cuda:0'), tensor([31], device='cuda:0'), tensor([16159], device='cuda:0'), tensor([198], device='cuda:0'), tensor([755], device='cuda:0'), tensor([8066], device='cuda:0'), tensor([33665], device='cuda:0'), tensor([1021], device='cuda:0'), tensor([262], device='cuda:0'), tensor([763], device='cuda:0'), tensor([25], device='cuda:0'), tensor([602], device='cuda:0'), tensor([23], device='cuda:0'), tensor([58], device='cuda:0'), tensor([845], device='cuda:0'), tensor([11], device='cuda:0'), tensor([220], device='cuda:0'), tensor([845], device='cuda:0'), tensor([60], device='cuda:0'), tensor([571], device='cuda:0'), tensor([14644], device='cuda:0'), tensor([1428], device='cuda:0'), tensor([345], device='cuda:0'), tensor([262], device='cuda:0'), tensor([1226], device='cuda:0'), tensor([2866], device='cuda:0'), tensor([25], device='cuda:0'), tensor([602], device='cuda:0'), tensor([23], device='cuda:0'), tensor([58], device='cuda:0'), tensor([845], device='cuda:0'), tensor([11], device='cuda:0'), tensor([220], device='cuda:0'), tensor([845], device='cuda:0'), tensor([60], device='cuda:0'), tensor([571], device='cuda:0'), tensor([14644], device='cuda:0'), tensor([1428], device='cuda:0'), tensor([345], device='cuda:0'), tensor([262], device='cuda:0'), tensor([4470], device='cuda:0'), tensor([25], device='cuda:0'), tensor([602], device='cuda:0'), tensor([23], device='cuda:0'), tensor([58], device='cuda:0'), tensor([845], device='cuda:0'), tensor([11], device='cuda:0'), tensor([220], device='cuda:0'), tensor([845], device='cuda:0'), tensor([60], device='cuda:0'), tensor([571], device='cuda:0'), tensor([14644], device='cuda:0'), tensor([1428], device='cuda:0'), tensor([345], device='cuda:0'), tensor([997], device='cuda:0'), tensor([262], device='cuda:0'), tensor([369], device='cuda:0'), tensor([602], device='cuda:0'), tensor([304], device='cuda:0'), tensor([13278], device='cuda:0'), tensor([7], device='cuda:0'), tensor([15], device='cuda:0'), tensor([11], device='cuda:0'), tensor([220], device='cuda:0'), tensor([845], device='cuda:0'), tensor([997], device='cuda:0'), tensor([286], device='cuda:0'), tensor([369], device='cuda:0'), tensor([503], device='cuda:0'), tensor([304], device='cuda:0'), tensor([13278], device='cuda:0'), tensor([7], device='cuda:0'), tensor([15], device='cuda:0'), tensor([11], device='cuda:0'), tensor([220], device='cuda:0'), tensor([845], device='cuda:0'), tensor([997], device='cuda:0'), tensor([310], device='cuda:0'), tensor([369], device='cuda:0'), tensor([597], device='cuda:0'), tensor([304], device='cuda:0'), tensor([13278], device='cuda:0'), tensor([7], device='cuda:0'), tensor([15], device='cuda:0'), tensor([11], device='cuda:0'), tensor([220], device='cuda:0'), tensor([845], device='cuda:0'), tensor([997], device='cuda:0'), tensor([394], device='cuda:0'), tensor([4470], device='cuda:0'), tensor([1004], device='cuda:0'), tensor([11], device='cuda:0'), tensor([503], device='cuda:0'), tensor([60], device='cuda:0'), tensor([1447], device='cuda:0'), tensor([763], device='cuda:0'), tensor([1004], device='cuda:0'), tensor([11], device='cuda:0'), tensor([597], device='cuda:0'), tensor([60], device='cuda:0'), tensor([353], device='cuda:0'), tensor([1226], device='cuda:0'), tensor([2866], device='cuda:0'), tensor([6874], device='cuda:0'), tensor([11], device='cuda:0'), tensor([503], device='cuda:0'), tensor([2595], device='cuda:0'), tensor([2], device='cuda:0'), tensor([4800], device='cuda:0'), tensor([584], device='cuda:0'), tensor([1893], device='cuda:0'), tensor([264], device='cuda:0'), tensor([9899], device='cuda:0'), tensor([311], device='cuda:0'), tensor([7417], device='cuda:0'), tensor([5178], device='cuda:0'), tensor([4999], device='cuda:0'), tensor([567], device='cuda:0'), tensor([1176], device='cuda:0'), tensor([3480], device='cuda:0'), tensor([311], device='cuda:0'), tensor([330], device='cuda:0'), tensor([3081], device='cuda:0'), tensor([53735], device='cuda:0'), tensor([702], device='cuda:0'), tensor([35203], device='cuda:0'), tensor([37511], device='cuda:0'), tensor([284], device='cuda:0'), tensor([30174], device='cuda:0'), tensor([3348], device='cuda:0'), tensor([10766], device='cuda:0'), tensor([33665], device='cuda:0'), tensor([11], device='cuda:0'), tensor([330], device='cuda:0'), tensor([16583], device='cuda:0'), tensor([33665], device='cuda:0'), tensor([646], device='cuda:0'), tensor([27742], device='cuda:0'), tensor([1158], device='cuda:0'), tensor([35203], device='cuda:0'), tensor([37511], device='cuda:0'), tensor([284], device='cuda:0'), tensor([84284], device='cuda:0'), tensor([5677], device='cuda:0'), tensor([3806], device='cuda:0'), tensor([3348], device='cuda:0'), tensor([336], device='cuda:0'), tensor([37511], device='cuda:0'), tensor([11], device='cuda:0'), tensor([330], device='cuda:0'), tensor([73], device='cuda:0'), tensor([597], device='cuda:0'), tensor([1158], device='cuda:0'), tensor([35203], device='cuda:0'), tensor([37511], device='cuda:0'), tensor([284], device='cuda:0'), tensor([84284], device='cuda:0'), tensor([5677], device='cuda:0'), tensor([3806], device='cuda:0'), tensor([3348], device='cuda:0'), tensor([336], device='cuda:0'), tensor([37511], device='cuda:0'), tensor([11], device='cuda:0'), tensor([330], device='cuda:0'), tensor([72], device='cuda:0'), tensor([597], device='cuda:0'), tensor([5240], device='cuda:0'), tensor([567], device='cuda:0'), tensor([1243], device='cuda:0'), tensor([220], device='cuda:0')]}\n"
     ]
    }
   ],
   "source": [
    "for a in ppo_trainer.dataloader:\n",
    "    print(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\" : 100,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"num_beams\": 4,\n",
    "    \"num_beam_groups\": 2,\n",
    "    \"diversity_penalty\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(ppo_trainer.dataloader)\n",
    "batch = next(it)\n",
    "query_tensors = batch[\"input_ids\"]\n",
    "query_tensors_formatted = [torch.cat(query_tensors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/workspaces/project/chipyard/.conda-env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:509: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "response_tensors = ppo_trainer.generate(query_tensors, batch_size=1, **generation_kwargs)\n",
    "batch[\"response\"] = [tokenizer.decode(r.squeeze(), skip_special_tokens=True) for r in response_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts: list[str] = [batch['query'][0] + r for r in batch[\"response\"]]\n",
    "rewards: list[Tensor] = get_rewards(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = rewards.index(max(rewards))\n",
    "response_tensors_formatted = [response_tensors[idx]]\n",
    "batch[\"selected_idx\"] = idx\n",
    "batch[\"selected_response\"] = batch[\"response\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "future> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/1999/02/22-rdf-syntax-ns#XMLLiteral>. <http://www.w3.org/1999/02/22-rdf-syntax-ns#XMLLiteral> <http://www.w3.org/1999/02/22-rdf-syntax-ns#datatype> <http://www.w3.org/\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"selected_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([21733,    29,   366,  1277,  1129,  2185,  1444,    18,  2726,    14,\n",
       "          2550,    24,    14,  2437,    14,  1313,  3880,  3013, 83879,    12,\n",
       "          4511,     2,  1337,    29,   366,  1277,  1129,  2185,  1444,    18,\n",
       "          2726,    14,  2550,    24,    14,  2437,    14,  1313,  3880,  3013,\n",
       "         83879,    12,  4511,     2, 10833, 17802,    29,   662,   366,  1277,\n",
       "          1129,  2185,  1444,    18,  2726,    14,  2550,    24,    14,  2437,\n",
       "            14,  1313,  3880,  3013, 83879,    12,  4511,     2, 10833, 17802,\n",
       "            29,   366,  1277,  1129,  2185,  1444,    18,  2726,    14,  2550,\n",
       "            24,    14,  2437,    14,  1313,  3880,  3013, 83879,    12,  4511,\n",
       "             2, 63509,    29,   366,  1277,  1129,  2185,  1444,    18,  2726,\n",
       "            14], device='cuda:0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(query_tensors_formatted, response_tensors_formatted, [max(rewards)])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 164.50 MiB is free. Process 64090 has 78.94 GiB memory in use. Of the allocated memory 77.96 GiB is allocated by PyTorch, and 342.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_tensors_formatted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_tensors_formatted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ppo_trainer\u001b[38;5;241m.\u001b[39mlog_stats(stats, batch, rewards)\n",
      "File \u001b[0;32m/workspaces/project/chipyard/.conda-env/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/project/chipyard/.conda-env/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:805\u001b[0m, in \u001b[0;36mPPOTrainer.step\u001b[0;34m(self, queries, responses, scores, response_masks)\u001b[0m\n\u001b[1;32m    796\u001b[0m             model_inputs \u001b[38;5;241m=\u001b[39m {k: mini_batch_dict[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m model_inputs_names}\n\u001b[1;32m    798\u001b[0m             logprobs, logits, vpreds, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatched_forward_pass(\n\u001b[1;32m    799\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    800\u001b[0m                 mini_batch_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    803\u001b[0m                 return_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    804\u001b[0m             )\n\u001b[0;32m--> 805\u001b[0m             train_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_minibatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmini_batch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmini_batch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmini_batch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmasks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmini_batch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madvantages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmini_batch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m             all_stats\u001b[38;5;241m.\u001b[39mappend(train_stats)\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# typically, early stopping is done at the epoch level\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/project/chipyard/.conda-env/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/project/chipyard/.conda-env/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1078\u001b[0m, in \u001b[0;36mPPOTrainer.train_minibatch\u001b[0;34m(self, old_logprobs, values, logprobs, logits, vpreds, mask, advantages, returns)\u001b[0m\n\u001b[1;32m   1074\u001b[0m loss_p, loss_v, train_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(\n\u001b[1;32m   1075\u001b[0m     old_logprobs, values, logits, vpreds, logprobs, mask, advantages, returns\n\u001b[1;32m   1076\u001b[0m )\n\u001b[1;32m   1077\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_p \u001b[38;5;241m+\u001b[39m loss_v\n\u001b[0;32m-> 1078\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_grad_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39msync_gradients:\n",
      "File \u001b[0;32m/workspaces/project/chipyard/.conda-env/lib/python3.10/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 164.50 MiB is free. Process 64090 has 78.94 GiB memory in use. Of the allocated memory 77.96 GiB is allocated by PyTorch, and 342.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "stats = ppo_trainer.step(query_tensors_formatted, response_tensors_formatted, [max(rewards)])\n",
    "ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = NUM_ITERATIONS\n",
    "for epoch in tqdm(range(epochs), \"epoch: \"):\n",
    "    for batch in tqdm(ppo_trainer.dataloader): \n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "        query_tensors_formatted = [torch.stack(query_tensors)]\n",
    "\n",
    "        #### Get response from SFTModel\n",
    "        response_tensors = ppo_trainer.generate(query_tensors, batch_size=1, **generation_kwargs)\n",
    "        batch[\"response\"] = [tokenizer.decode(r.squeeze(), skip_special_tokens=True) for r in response_tensors]\n",
    "\n",
    "        #### Compute reward score\n",
    "        texts: list[str] = [batch['query'][0] + r for r in batch[\"response\"]]\n",
    "        rewards: list[Tensor] = get_rewards(texts)\n",
    "\n",
    "        idx = rewards.index(max(rewards))\n",
    "        response_tensors_formatted = [response_tensors[idx]]\n",
    "        batch[\"selected_idx\"] = idx\n",
    "        batch[\"selected_response\"] = batch[\"response\"][idx]\n",
    "\n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(query_tensors_formatted, response_tensors_formatted, [max(rewards)])\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save model\n",
    "ppo_trainer.save_pretrained(\"my_ppo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
